{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DrBg_S9K1bYP"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8SWaiA4XAMU"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "id": "OmkpCVtXjC3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "mrqa = load_dataset(\"enriquesaou/mrqa-squadded-sample\")"
      ],
      "metadata": {
        "id": "OM1nfZW-lBe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrqa"
      ],
      "metadata": {
        "id": "0ITxwE7KnR5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "RrIcZYGnoqyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://huggingface.co/FacebookAI/roberta-base\n",
        "base_model_id = \"VMware/roberta-base-mrqa\"#\"FacebookAI/roberta-base\"#"
      ],
      "metadata": {
        "id": "PxB7Ppg9oxcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(base_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "fhKy6AxJoqYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "stride = 128"
      ],
      "metadata": {
        "id": "19wXHGzv9u1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\n",
        "def preprocess_training(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        sample_idx = sample_map[i]\n",
        "        answer = answers[sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "GB5eeDalq-9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_validation(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        inputs = tokenizer(\n",
        "            questions,\n",
        "            examples[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=max_length,\n",
        "            stride=stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "        inputs[\"example_id\"] = []\n",
        "\n",
        "        for i in range(len(inputs[\"input_ids\"])):\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "            context_index = 1\n",
        "\n",
        "            sample_index = sample_mapping[i]\n",
        "            inputs[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "            inputs[\"offset_mapping\"][i] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(inputs[\"offset_mapping\"][i])\n",
        "            ]\n",
        "\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "gU1sSXCTPPAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mrqa = mrqa['train'].map(\n",
        "    preprocess_training,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa['train'].column_names,\n",
        ")\n",
        "\n",
        "val_mrqa = mrqa['validation'].map(\n",
        "    preprocess_validation,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa['validation'].column_names,\n",
        ")\n",
        "\n",
        "mrqa, train_mrqa, val_mrqa"
      ],
      "metadata": {
        "id": "E2Bl1kWLsNJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "WAX-g2m3zwta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model_id = \"roberta-vmw-mrqa-waw\""
      ],
      "metadata": {
        "id": "gl43N4BPzvvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "model.train()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-6,\n",
        "    per_device_train_batch_size=20,\n",
        "    per_device_eval_batch_size=20,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_mrqa,\n",
        "    eval_dataset=val_mrqa,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "WjdDPl24z03R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "FCVP6_lT3gVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "point = \"checkpoint-1500\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"roberta-vmw-mrqa-plus/\"+point)\n",
        "model.push_to_hub(\"roberta-vmw-mrqa-plus-\"+point)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Dh-YXNNvH5Hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
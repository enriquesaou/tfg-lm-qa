{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "collapsed_sections": [
        "AlxBnOMXkHtb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnPXq6fcTXMW"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off"
      ],
      "metadata": {
        "id": "ldI7r37NTmVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://huggingface.co/google/flan-t5-small\n",
        "base_model_id = \"google/flan-t5-base\"#\"google-t5/t5-small\"#\"google-t5/t5-base\"#\"google-t5/t5-small\"#\"google-t5/t5-base\"#\"google/t5-v1_1-small\" #\"google/flan-t5-small\""
      ],
      "metadata": {
        "id": "ysg9XD3jTtuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(base_model_id)\n",
        "tokenizer = T5Tokenizer.from_pretrained(base_model_id)\n",
        "tokenizer.add_special_tokens({'sep_token': \"<s>\"})"
      ],
      "metadata": {
        "id": "reEE1xbHTyW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.sep_token)"
      ],
      "metadata": {
        "id": "3fi-ET8PZd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "mrqa = load_dataset(\"enriquesaou/mrqa-squadded-sample\")"
      ],
      "metadata": {
        "id": "_AasoE2lToTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrqa"
      ],
      "metadata": {
        "id": "c64rrKovTpy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "stride = 128"
      ],
      "metadata": {
        "id": "jgi8ElASl7Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_seq2seq_qa.py\n",
        "def generate_input(_question, _context):\n",
        "    return \" \".join([\"question:\", _question.strip(), tokenizer.sep_token, \"context:\", _context.strip()])\n",
        "\n",
        "def preprocess_mrqa_batch(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "        answers = examples[\"answers\"]\n",
        "\n",
        "        inputs = [generate_input(question, context) for question, context in zip(questions, contexts)]\n",
        "        targets = [answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"\" for answer in answers]\n",
        "        return inputs, targets\n",
        "\n",
        "def preprocess_training(examples, _max_length=max_length, _stride=stride, padding=\"max_length\", truncation=True):\n",
        "    inputs, targets = preprocess_mrqa_batch(examples)\n",
        "\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=_max_length,\n",
        "                             stride=_stride,\n",
        "                             padding=padding,\n",
        "                             truncation=truncation)\n",
        "    labels = tokenizer(text_target=targets,\n",
        "                       max_length=_max_length,\n",
        "                       stride=_stride,\n",
        "                       padding=padding,\n",
        "                       truncation=truncation)\n",
        "\n",
        "    # Replace tokenizer.pad_token_id in the labels to ignore padding in the loss\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "nf1r6taRkb4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_validation(examples, _max_length=max_length, _stride=stride, padding=\"max_length\", truncation=True):\n",
        "        inputs, targets = preprocess_mrqa_batch(examples)\n",
        "\n",
        "        model_inputs = tokenizer(inputs,\n",
        "                                 max_length=_max_length,\n",
        "                                 padding=padding,\n",
        "                                 truncation=truncation,\n",
        "                                 return_overflowing_tokens=True,\n",
        "                                 return_offsets_mapping=True)\n",
        "        labels = tokenizer(text_target=targets,\n",
        "                           max_length=_max_length,\n",
        "                           padding=padding,\n",
        "                           truncation=truncation)\n",
        "\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "        sample_mapping = model_inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "        model_inputs[\"example_id\"] = []\n",
        "        labels_out = []\n",
        "\n",
        "        for i in range(len(model_inputs[\"input_ids\"])):\n",
        "            sample_index = sample_mapping[i]\n",
        "            model_inputs[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "            labels_out.append(labels[\"input_ids\"][sample_index])\n",
        "\n",
        "        model_inputs[\"labels\"] = labels_out\n",
        "        return model_inputs"
      ],
      "metadata": {
        "id": "ljpxKDSxORwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mrqa = mrqa['train'].map(\n",
        "    preprocess_training,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa['train'].column_names,\n",
        ")\n",
        "\n",
        "val_mrqa = mrqa['validation'].map(\n",
        "    preprocess_training,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa['validation'].column_names,\n",
        ")\n",
        "\n",
        "mrqa, train_mrqa, val_mrqa"
      ],
      "metadata": {
        "id": "oVFmh9TU6K1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plots"
      ],
      "metadata": {
        "id": "AlxBnOMXkHtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = tokenizer('this is T5 tokenizer! is dog the same as dogs?')\n",
        "result"
      ],
      "metadata": {
        "id": "fGHT0hK3KUmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenizer))\n",
        "print(tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "_-GY-3TTOjVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in result['input_ids']:\n",
        "    print(tokenizer.decode(id))\n",
        "\n",
        "print(tokenizer.decode(result['input_ids'], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "_N0JorI3KkSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tok_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tok_dataset]\n",
        "    print(len(lengths))\n",
        "    print(lengths)\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cwM3_Kg9sGMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_data_lengths(tokenized_mrqa['train']), plot_data_lengths(tokenized_mrqa['test'])"
      ],
      "metadata": {
        "id": "caj_Mg4PsL-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train"
      ],
      "metadata": {
        "id": "Ij3zloCMkKfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model_id = \"flan-t5-base-mrqa-16\""
      ],
      "metadata": {
        "id": "kpXY4Xavwhrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model.train()\n",
        "\n",
        "\"\"\"\n",
        "# base\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    #eval_strategy=\"steps\",\n",
        "    #max_steps=5,\n",
        "    do_train=True,\n",
        "    fp16=True, #https://discuss.huggingface.co/t/training-loss-0-0-validation-loss-nan/27950/4\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=5, #overfit at >3\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    gradient_accumulation_steps=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# flan base 2\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    #eval_strategy=\"steps\",\n",
        "    #max_steps=5,\n",
        "    do_train=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=5,#4\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=6,#4\n",
        "    per_device_eval_batch_size=6,#4\n",
        "    gradient_accumulation_steps=3,#2\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# flan small\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    #eval_strategy=\"steps\",\n",
        "    #max_steps=5,\n",
        "    do_train=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=7,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    gradient_accumulation_steps=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# flan base\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    #eval_strategy=\"steps\",\n",
        "    #max_steps=5,\n",
        "    do_train=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "# small\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    #eval_strategy=\"steps\",\n",
        "    #max_steps=5,\n",
        "    do_train=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=8,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=14,\n",
        "    per_device_eval_batch_size=14,\n",
        "    gradient_accumulation_steps=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_mrqa,\n",
        "    eval_dataset=val_mrqa,\n",
        "    #eval_examples=tokenized_mrqa[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jmxM1l8y0Qjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "0dJrIjbG0GgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "AElU5TCifcKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5xHZ9L_ffwmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "i7Hsg7f-gIAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "id": "aoJOXbcikUX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoXowavqj4vO"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "#!pip install -q transformers==4.29.2 --progress-bar off\n",
        "!pip install accelerate --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off\n",
        "!pip install -q -U bitsandbytes --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "mrqa = load_dataset(\"enriquesaou/mrqa-squadded-sample\")"
      ],
      "metadata": {
        "id": "C2YW4MeqlTut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrqa"
      ],
      "metadata": {
        "id": "G-ebIhSnlamC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "d2imyuIMJ4-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "638VVEmDlmXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\"\"\" # quant to 4 bits\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype='float16',\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Mb4GM2RLke7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "id": "T_fyWY1Dkr76",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\", # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
        "    add_eos_token=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token # https://kaitchup.substack.com/p/phi-2-a-small-model-easy-to-fine"
      ],
      "metadata": {
        "id": "WVL_Az03kue4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cqa(context, question):\n",
        "    return \"Answer the question extracting from the context below.\\nContext: \" + context + \"\\nQuestion: \" + question + \"\\nAnswer: \""
      ],
      "metadata": {
        "id": "yW167ui_b-cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompt(data_point):\n",
        "    full_prompt = format_cqa(data_point['context'], data_point['question']) + data_point['answers']['text'][0]\n",
        "\n",
        "    result = tokenizer(full_prompt)\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "#tokenized_dataset = mrqa.map(tokenize_prompt,\n",
        "#                             remove_columns=mrqa['train'].column_names)"
      ],
      "metadata": {
        "id": "v-LDtpI6Ka44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mrqa, tokenized_dataset"
      ],
      "metadata": {
        "id": "0WKfMFS1VUeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that tokenization is correct\n",
        "#untokenized_text = tokenizer.decode(tokenized_dataset['train'][0]['input_ids'], skip_special_tokens=True)\n",
        "#print(untokenized_text)"
      ],
      "metadata": {
        "id": "z4yCUKxyKcZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tok_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tok_dataset]\n",
        "    print(len(lengths))\n",
        "    print(lengths)\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "iqeZaZ9oLT05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_data_lengths(tokenized_dataset['train']), plot_data_lengths(tokenized_dataset['test'])"
      ],
      "metadata": {
        "id": "AQu5cMuvbIM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1200\n",
        "\n",
        "def tokenize_prompt(data_point):\n",
        "    full_prompt = format_cqa(data_point['context'], data_point['question']) + data_point['answers']['text'][0]\n",
        "\n",
        "    result = tokenizer(full_prompt,\n",
        "                       truncation=True,\n",
        "                       max_length=max_length,\n",
        "                       padding=\"max_length\")\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "tokenized_dataset = mrqa.map(tokenize_prompt,\n",
        "                             remove_columns=mrqa['train'].column_names)"
      ],
      "metadata": {
        "id": "mzknfIE6a7I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model.train()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "#phi3 adapter\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "#model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "K_PtvP6okf62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model_id = \"enriquesaou/phi-3-mrqa\""
      ],
      "metadata": {
        "id": "6eoJHkd_La5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=my_model_id,\n",
        "    per_device_train_batch_size=4,#4, # lower for less memory\n",
        "    gradient_accumulation_steps=1, # effective batch size: 1*4 (but training is slowed)\n",
        "    max_steps=300,\n",
        "    warmup_ratio=.03,\n",
        "    learning_rate=3e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    #do_eval=True,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    args=training_arguments,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8j-jmXRnLelG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "gpjPTRWvLgbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
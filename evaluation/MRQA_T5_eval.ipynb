{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "id": "Sn6yD9ehPM99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "#!pip install -q -U git+https://github.com/huggingface/accelerate.git --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off"
      ],
      "metadata": {
        "id": "N039a-2pPSqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "base_model_id = \"google/flan-t5-base\"#\"google/flan-t5-base\"#\"google-t5/t5-base\"# \"google/flan-t5-small\"#\"google-t5/t5-small\"#\"google/t5-v1_1-small\"#\"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "tokenizer.add_special_tokens({'sep_token': \"<s>\"})"
      ],
      "metadata": {
        "id": "2iH7F4S5QF10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "stride = 128"
      ],
      "metadata": {
        "id": "MggYr8MrQ7pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-dSeind4p9r"
      },
      "outputs": [],
      "source": [
        "def generate_input(_question, _context):\n",
        "    return \" \".join([\"question:\", _question.strip(), tokenizer.sep_token, \"context:\", _context.strip(), tokenizer.sep_token,  \"answer:\"])\n",
        "\n",
        "def preprocess_mrqa_batch(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "        answers = examples[\"answers\"]\n",
        "\n",
        "        inputs = [generate_input(question, context) for question, context in zip(questions, contexts)]\n",
        "        targets = [answer['text'][0] if len(answer) > 0 else \"\" for answer in answers]\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "# validation preprocessing\n",
        "def preprocess_validation(examples):\n",
        "    inputs, targets = preprocess_mrqa_batch(examples)\n",
        "\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=max_length,\n",
        "                             stride=stride,\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             return_overflowing_tokens=True,\n",
        "                             return_offsets_mapping=True)\n",
        "    labels = tokenizer(text_target=targets,\n",
        "                       max_length=max_length,\n",
        "                       stride=stride,\n",
        "                       padding=\"max_length\",\n",
        "                       truncation=True)\n",
        "\n",
        "    # Replace tokenizer.pad_token_id in the labels to ignore padding in the loss\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    # examples with long context give us several features -> map feature to example\n",
        "    sample_mapping = model_inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # convert predictions to substrings of the context for evaluation\n",
        "    model_inputs[\"example_id\"] = []\n",
        "    # Augment the overflowing tokens to the labels\n",
        "    labels_out = []\n",
        "    for i in range(len(model_inputs[\"input_ids\"])):\n",
        "        # an example can give many spans -> take index of the example containing the span\n",
        "        sample_index = sample_mapping[i]\n",
        "        model_inputs[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "        labels_out.append(labels[\"input_ids\"][sample_index])\n",
        "\n",
        "    model_inputs[\"labels\"] = labels_out\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# source: https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\n",
        "def postprocess_qa_predictions(examples, features, predictions):\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    # Replace -100s used for padding as we can't decode them\n",
        "    #predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    predictions = [np.where(p != -100, p, tokenizer.pad_token_id) for p in predictions]\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    feature_per_example = {example_id_to_index[feature[\"example_id\"]]: i for i, feature in enumerate(features)}\n",
        "    all_predictions = {}\n",
        "    for example_index, example in enumerate(examples):\n",
        "        # This is the index of the feature associated to the current example.\n",
        "        feature_index = feature_per_example[example_index]\n",
        "        all_predictions[example[\"id\"]] = decoded_preds[feature_index]\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "v1XC29DA5LYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "split =\"test\" #\"validation\" #\n",
        "mrqa_eval = load_dataset(\"enriquesaou/mrqa-squadded-sample\", split=split)\n",
        "\n",
        "## preprocess eval dataset\n",
        "eval_set = mrqa_eval.map(\n",
        "    preprocess_validation,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa_eval.column_names,\n",
        ")\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "pE4F3Dnu8_xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set_for_model"
      ],
      "metadata": {
        "id": "Gj1lxVUz9F4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/mrqa/MRQA-Shared-Task-2019/blob/master/mrqa_official_eval.py\n",
        "\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_predictions(prediction_file):\n",
        "    with open(prediction_file) as f:\n",
        "        predictions = json.load(f)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def read_answers(gold_file):\n",
        "    answers = {}\n",
        "    with gzip.open(gold_file, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = json.loads(line)\n",
        "            if i == 0 and 'header' in example:\n",
        "                continue\n",
        "            for qa in example['qas']:\n",
        "                answers[qa['id']] = qa['answers']\n",
        "    return answers\n",
        "\n",
        "\n",
        "def evaluate_predictions(answers, predictions, skip_no_answer=False):\n",
        "    f1 = exact_match = total = 0\n",
        "\n",
        "    for qid, ground_truths in answers.items():\n",
        "        if qid not in predictions:\n",
        "            if not skip_no_answer:\n",
        "                message = 'Unanswered question %s will receive score 0.' % qid\n",
        "                print(message)\n",
        "                total += 1\n",
        "            continue\n",
        "        total += 1\n",
        "        prediction = predictions[qid]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "t5EQPyTf9JKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_evaluate = [\"enriquesaou/flan-t5-base-mrqa-16\"]"
      ],
      "metadata": {
        "id": "LbmaJ_F-yY0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# use cuda for faster computation\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "    # load model and evaluate\n",
        "    model_for_eval = T5ForConditionalGeneration.from_pretrained(model_id).to(device)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(eval_set_for_model, batch_size=32)\n",
        "    outputs = []\n",
        "    for batch in tqdm(dataloader):\n",
        "      outs = model_for_eval.generate(input_ids=batch['input_ids'].to(device),\n",
        "                                     attention_mask=batch['attention_mask'].to(device),\n",
        "                                     max_new_tokens=16)\n",
        "      outputs.extend(outs)\n",
        "\n",
        "    outputs = [o.to('cpu') for o in outputs]\n",
        "\n",
        "    # postprocess the predictions\n",
        "    all_predictions = postprocess_qa_predictions(\n",
        "        examples=mrqa_eval,\n",
        "        features=eval_set,\n",
        "        predictions=outputs)\n",
        "\n",
        "    # compute metrics\n",
        "    answers = mrqa_eval.to_dict()\n",
        "    answers = {id: aws['text'] for id, aws in zip(answers['id'], answers['answers'])}\n",
        "    metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "    print(model_id, split, json.dumps(metrics))\n",
        "\n",
        "    \"\"\"\n",
        "    for k in answers.keys():\n",
        "        print(all_predictions[k], answers[k])\n",
        "        metrics = evaluate_predictions({k: answers[k]}, {k: all_predictions[k]})\n",
        "        print(json.dumps(metrics))\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "e0gogRim9PZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
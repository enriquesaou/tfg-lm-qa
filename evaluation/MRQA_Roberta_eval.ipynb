{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "id": "rolJPkBAzt25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "id": "SaxcRFW7zwUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "base_model_id = \"VMware/roberta-base-mrqa\"#\"FacebookAI/roberta-base\"#\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "FXLqJcKjzKu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "split = \"test\"# \"test\"\"validation\"#\n",
        "mrqa_eval = load_dataset(\"enriquesaou/mrqa-squadded-sample\", split=split)"
      ],
      "metadata": {
        "id": "SOL0bRid0lwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "stride = 128"
      ],
      "metadata": {
        "id": "cKdQ5-AwxP0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\n",
        "def preprocess_validation(examples):\n",
        "    examples[\"question\"] = [q.strip() for q in examples[\"question\"]]\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # examples with long context give us several features -> map feature to example\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # convert predictions to substrings of the context for evaluation\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1\n",
        "\n",
        "        # an example can give many spans -> take index of the example containing the span\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # offset_mapping to None if they are not part of the context\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "3RBtOGD60ixy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# source: https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\n",
        "def postprocess_qa_predictions(\n",
        "    examples,\n",
        "    features,\n",
        "    predictions: Tuple[np.ndarray, np.ndarray],\n",
        "    version_2_with_negative: bool = False,\n",
        "    n_best_size: int = 20,\n",
        "    max_answer_length: int = 30,\n",
        "    null_score_diff_threshold: float = 0.0,\n",
        "    output_dir: Optional[str] = None,\n",
        "    prefix: Optional[str] = None,\n",
        "    log_level: Optional[int] = logging.WARNING,\n",
        "):\n",
        "    if len(predictions) != 2:\n",
        "        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "\n",
        "    if len(predictions[0]) != len(features):\n",
        "        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n",
        "\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    all_predictions = collections.OrderedDict()\n",
        "    all_nbest_json = collections.OrderedDict()\n",
        "    if version_2_with_negative:\n",
        "        scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    logger.setLevel(log_level)\n",
        "    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_prediction = None\n",
        "        prelim_predictions = []\n",
        "\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
        "            # available in the current feature.\n",
        "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            feature_null_score = start_logits[0] + end_logits[0]\n",
        "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
        "                min_null_prediction = {\n",
        "                    \"offsets\": (0, 0),\n",
        "                    \"score\": feature_null_score,\n",
        "                    \"start_logit\": start_logits[0],\n",
        "                    \"end_logit\": end_logits[0],\n",
        "                }\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or len(offset_mapping[start_index]) < 2\n",
        "                        or offset_mapping[end_index] is None\n",
        "                        or len(offset_mapping[end_index]) < 2\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
        "                    # provided).\n",
        "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
        "                        continue\n",
        "\n",
        "                    prelim_predictions.append(\n",
        "                        {\n",
        "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"start_logit\": start_logits[start_index],\n",
        "                            \"end_logit\": end_logits[end_index],\n",
        "                        }\n",
        "                    )\n",
        "        if version_2_with_negative and min_null_prediction is not None:\n",
        "            # Add the minimum null prediction\n",
        "            prelim_predictions.append(min_null_prediction)\n",
        "            null_score = min_null_prediction[\"score\"]\n",
        "\n",
        "        # Only keep the best `n_best_size` predictions.\n",
        "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "\n",
        "        # Add back the minimum null prediction if it was removed because of its low score.\n",
        "        if (\n",
        "            version_2_with_negative\n",
        "            and min_null_prediction is not None\n",
        "            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n",
        "        ):\n",
        "            predictions.append(min_null_prediction)\n",
        "\n",
        "        # Use the offsets to gather the answer text in the original context.\n",
        "        context = example[\"context\"]\n",
        "        for pred in predictions:\n",
        "            offsets = pred.pop(\"offsets\")\n",
        "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
        "\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "        # failure.\n",
        "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
        "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
        "\n",
        "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
        "        # the LogSumExp trick).\n",
        "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        probs = exp_scores / exp_scores.sum()\n",
        "\n",
        "        # Include the probabilities in our predictions.\n",
        "        for prob, pred in zip(probs, predictions):\n",
        "            pred[\"probability\"] = prob\n",
        "\n",
        "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
        "        if not version_2_with_negative:\n",
        "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
        "        else:\n",
        "            # Otherwise we first need to find the best non-empty prediction.\n",
        "            i = 0\n",
        "            while predictions[i][\"text\"] == \"\":\n",
        "                i += 1\n",
        "            best_non_null_pred = predictions[i]\n",
        "\n",
        "            # Then we compare to the null prediction using the threshold.\n",
        "            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
        "            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n",
        "            if score_diff > null_score_diff_threshold:\n",
        "                all_predictions[example[\"id\"]] = \"\"\n",
        "            else:\n",
        "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
        "\n",
        "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
        "        all_nbest_json[example[\"id\"]] = [\n",
        "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
        "            for pred in predictions\n",
        "        ]\n",
        "\n",
        "    # If we have an output_dir, let's save all those dicts.\n",
        "    if output_dir is not None:\n",
        "        if not os.path.isdir(output_dir):\n",
        "            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n",
        "\n",
        "        prediction_file = os.path.join(\n",
        "            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n",
        "        )\n",
        "        nbest_file = os.path.join(\n",
        "            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n",
        "        )\n",
        "        if version_2_with_negative:\n",
        "            null_odds_file = os.path.join(\n",
        "                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Saving predictions to {prediction_file}.\")\n",
        "        with open(prediction_file, \"w\") as writer:\n",
        "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n",
        "        with open(nbest_file, \"w\") as writer:\n",
        "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "        if version_2_with_negative:\n",
        "            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n",
        "            with open(null_odds_file, \"w\") as writer:\n",
        "                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "_NC75bjC0cug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess eval dataset\n",
        "eval_set = mrqa_eval.map(\n",
        "    preprocess_validation,\n",
        "    batched=True,\n",
        "    remove_columns=mrqa_eval.column_names,\n",
        ")\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "w9bmEZHv0adw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set_for_model"
      ],
      "metadata": {
        "id": "faZxpdbM0Xr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/mrqa/MRQA-Shared-Task-2019/blob/master/mrqa_official_eval.py\n",
        "\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_predictions(prediction_file):\n",
        "    with open(prediction_file) as f:\n",
        "        predictions = json.load(f)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def read_answers(gold_file):\n",
        "    answers = {}\n",
        "    with gzip.open(gold_file, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = json.loads(line)\n",
        "            if i == 0 and 'header' in example:\n",
        "                continue\n",
        "            for qa in example['qas']:\n",
        "                answers[qa['id']] = qa['answers']['text']\n",
        "    return answers\n",
        "\n",
        "\n",
        "def evaluate_predictions(answers, predictions, skip_no_answer=False):\n",
        "    f1 = exact_match = total = 0\n",
        "    for qid, ground_truths in answers.items():\n",
        "        if qid not in predictions:\n",
        "            if not skip_no_answer:\n",
        "                message = 'Unanswered question %s will receive score 0.' % qid\n",
        "                print(message)\n",
        "                total += 1\n",
        "            continue\n",
        "        total += 1\n",
        "        prediction = predictions[qid]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "hMlS2n3G0VVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_evaluate = [\"enriquesaou/roberta-vmw-mrqa-waw\"]#[\"enriquesaou/roberta-vmw-mrqa-plus-checkpoint-1000\",\"enriquesaou/roberta-vmw-mrqa-plus-checkpoint-1500\", \"enriquesaou/roberta-vmw-mrqa-plus-checkpoint-2000\",\"enriquesaou/roberta-vmw-mrqa-plus-checkpoint-2500\"]#\"enriquesaou/roberta-mrqa-plus\"]#[\"enriquesaou/roberta-vmw-mrqa-s\"]#[\"enriquesaou/roberta-vmw-mrqa\"]#, \"VMware/roberta-base-mrqa\"]\n",
        "output_dir = \"predictions/\""
      ],
      "metadata": {
        "id": "ovJ60nuRz-9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def split_batch(batch, chunk_size):\n",
        "    keys = list(batch.keys())\n",
        "    length = batch[keys[0]].size(0)\n",
        "    for start in range(0, length, chunk_size):\n",
        "        end = min(start + chunk_size, length)\n",
        "        yield {k: v[start:end] for k, v in batch.items()}\n",
        "\n",
        "def accumulate_predictions(predictions, start_logits_chunk, end_logits_chunk):\n",
        "    predictions['start_logits'].append(start_logits_chunk)\n",
        "    predictions['end_logits'].append(end_logits_chunk)\n",
        "    return predictions\n",
        "\n",
        "# prepare batches\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "\n",
        "chunk_size = 8  # adjust to memory\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "    model_for_eval = AutoModelForQuestionAnswering.from_pretrained(model_id).to(device)\n",
        "\n",
        "    all_start_logits = []\n",
        "    all_end_logits = []\n",
        "\n",
        "    # chunk processing\n",
        "    for sub_batch in tqdm(split_batch(batch, chunk_size)):\n",
        "        with torch.no_grad():\n",
        "            outputs = model_for_eval(**sub_batch)\n",
        "\n",
        "        all_start_logits.append(outputs.start_logits.cpu().numpy())\n",
        "        all_end_logits.append(outputs.end_logits.cpu().numpy())\n",
        "\n",
        "    start_logits = np.concatenate(all_start_logits, axis=0)\n",
        "    end_logits = np.concatenate(all_end_logits, axis=0)\n",
        "\n",
        "    # for inspection\n",
        "    outdir = f\"{output_dir}/{model_id}\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    # postprocess the predictions (logits to actual answers)\n",
        "    all_predictions = postprocess_qa_predictions(\n",
        "        examples=mrqa_eval,\n",
        "        features=eval_set,\n",
        "        predictions=(start_logits, end_logits),\n",
        "        version_2_with_negative=False,  # non squad-v2\n",
        "        output_dir=outdir,\n",
        "    )\n",
        "\n",
        "    # compute metrics\n",
        "    answers = {qid: aws['text'] for qid, aws in zip(mrqa_eval['id'], mrqa_eval['answers'])}\n",
        "    metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "    print(model_id, split, json.dumps(metrics))"
      ],
      "metadata": {
        "id": "QNKbV8mU0C3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#deprecated"
      ],
      "metadata": {
        "id": "qHIuTTgD5a-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "import os\n",
        "\n",
        "\n",
        "# prepare batches. Use cuda for faster computation\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "    # load model and evaluate\n",
        "    model_for_eval = AutoModelForQuestionAnswering.from_pretrained(model_id).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_for_eval(**batch)\n",
        "\n",
        "    # retrieve results\n",
        "    start_logits = outputs.start_logits.cpu().numpy()\n",
        "    end_logits = outputs.end_logits.cpu().numpy()\n",
        "\n",
        "    # output dir for inspection\n",
        "    outdir = f\"{output_dir}/{model_id}\"\n",
        "    if not os.path.exists(outdir): os.makedirs(outdir)\n",
        "\n",
        "    # postprocess the predictions (logits to actual answers)\n",
        "    all_predictions = postprocess_qa_predictions(\n",
        "        examples=mrqa_eval,\n",
        "        features=eval_set,\n",
        "        predictions=(start_logits, end_logits),\n",
        "        version_2_with_negative=False, # non squad-v2\n",
        "        output_dir=outdir,\n",
        "    )\n",
        "\n",
        "    # compute metrics\n",
        "    #answers = mrqa_eval.remove_columns([\"subset\", \"context\", \"context_tokens\", \"question\", \"question_tokens\", \"detected_answers\"]).to_dict()\n",
        "    answers = {qid: aws for qid, aws in zip(mrqa_eval['id'], mrqa_eval['answers']['text'])}\n",
        "    metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "    print(model_id, json.dumps(metrics))\n",
        "\n",
        "\n",
        "    for k in answers.keys():\n",
        "        print(all_predictions[k], answers[k])\n",
        "        metrics = evaluate_predictions({k: answers[k]}, {k: all_predictions[k]})\n",
        "        print(json.dumps(metrics))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E9xgya0jz25w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "id": "Sn6yD9ehPM99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off\n",
        "!pip install -q -U bitsandbytes --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git --progress-bar off"
      ],
      "metadata": {
        "id": "N039a-2pPSqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"#\"microsoft/phi-3-mini-4k-instruct\"#\"microsoft/phi-2\"\n",
        "# eval tokenizer does not have eos token and padding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        "    #truncate=True,\n",
        "    #padding_side=\"left\", # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
        ")\n",
        "#tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "2iH7F4S5QF10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "split = \"validation\"#\"test\"#\n",
        "mrqa_eval = load_dataset(\"enriquesaou/mrqa-squadded-sample\", split=split)"
      ],
      "metadata": {
        "id": "pE4F3Dnu8_xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/mrqa/MRQA-Shared-Task-2019/blob/master/mrqa_official_eval.py\n",
        "\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_predictions(prediction_file):\n",
        "    with open(prediction_file) as f:\n",
        "        predictions = json.load(f)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def read_answers(gold_file):\n",
        "    answers = {}\n",
        "    with gzip.open(gold_file, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = json.loads(line)\n",
        "            if i == 0 and 'header' in example:\n",
        "                continue\n",
        "            for qa in example['qas']:\n",
        "                answers[qa['qid']] = qa['answers']\n",
        "    return answers\n",
        "\n",
        "\n",
        "def evaluate_predictions(answers, predictions, skip_no_answer=False):\n",
        "    f1 = exact_match = total = 0\n",
        "\n",
        "    for qid, ground_truths in answers.items():\n",
        "        if qid not in predictions:\n",
        "            if not skip_no_answer:\n",
        "                message = 'Unanswered question %s will receive score 0.' % qid\n",
        "                print(message)\n",
        "                total += 1\n",
        "            continue\n",
        "        total += 1\n",
        "        prediction = predictions[qid]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "t5EQPyTf9JKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"], #=\"all-linear\",\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "BKF3Vy8IKdXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cqa(context, question):\n",
        "    return \"Answer the question extracting from the context below.\\nContext: \" + context + \"\\nQuestion: \" + question + \"\\nAnswer: \""
      ],
      "metadata": {
        "id": "IJLlvqN3Xqn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_generate(test_model, prompt, new_tokens=16):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=new_tokens)\n",
        "        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "PI17VKLWXhpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_evaluate = [\"enriquesaou/phi-2-mrqa\"]"
      ],
      "metadata": {
        "id": "LbmaJ_F-yY0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "new_tok = 5\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # load peft if not base\n",
        "    if model_id != base_model_id:\n",
        "      # note that base_model may be modified in place\n",
        "      model_for_eval = PeftModel.from_pretrained(base_model, model_id)\n",
        "    else: model_for_eval = base_model\n",
        "\n",
        "    model_for_eval.to('cuda').eval()\n",
        "\n",
        "    all_predictions = {}\n",
        "    for example in tqdm(mrqa_eval):\n",
        "      prompt = format_cqa(example['context'], example['question'])\n",
        "      outs = tokenize_and_generate(model_for_eval, prompt, new_tokens=new_tok)\n",
        "      outs = outs.replace(prompt, '')\n",
        "      outs = outs.split('Answer:')[1] if 'Answer:' in outs else outs\n",
        "      all_predictions[example['id']] = outs.strip()\n",
        "\n",
        "\n",
        "    # compute metrics\n",
        "    answers = mrqa_eval.to_dict()\n",
        "    answers = {id: aws['text'] for id, aws in zip(answers['id'], answers['answers'])}\n",
        "    metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "    print(model_id, split, json.dumps(metrics))\n",
        "\n",
        "    \"\"\"\n",
        "    for k in answers.keys():\n",
        "        print(all_predictions[k], answers[k])\n",
        "        metrics = evaluate_predictions({k: answers[k]}, {k: all_predictions[k]})\n",
        "        print(json.dumps(metrics))\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "e0gogRim9PZ2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test token length"
      ],
      "metadata": {
        "id": "XwtFvuzbU86N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "models_to_evaluate = [\"enriquesaou/phi2-mrqa\"]\n",
        "\n",
        "mrqa_eval = mrqa_eval.shuffle(seed=27).select(range(150))\n",
        "\n",
        "tk = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 40, 50]\n",
        "em = []\n",
        "f1 = []\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # load peft if not base\n",
        "    if model_id != base_model_id:\n",
        "      # note that base_model may be modified in place\n",
        "      model_for_eval = PeftModel.from_pretrained(base_model, model_id)\n",
        "    else:\n",
        "      model_for_eval = base_model\n",
        "\n",
        "    model_for_eval.eval()\n",
        "\n",
        "    for new_tok in tk:\n",
        "      all_predictions = {}\n",
        "      for example in tqdm(mrqa_eval):\n",
        "        prompt = format_cqa(example['context'], example['question'])\n",
        "        outs = tokenize_and_generate(model_for_eval, prompt, new_tokens=new_tok).replace(prompt, '')\n",
        "        outs = outs.split('Answer:')[1] if 'Answer:' in outs else outs\n",
        "        all_predictions[example['id']] = outs.strip()\n",
        "\n",
        "\n",
        "      # compute metrics\n",
        "      answers = mrqa_eval.to_dict()\n",
        "      answers = {qid: aws['text'] for qid, aws in zip(answers['id'], answers['answers'])}\n",
        "      metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "      em.append(metrics['exact_match'])\n",
        "      f1.append(metrics['f1'])\n",
        "\n",
        "      print(model_id, json.dumps(metrics))\n",
        "\n",
        "      \"\"\"\n",
        "      for k in answers.keys():\n",
        "          print(all_predictions[k], answers[k])\n",
        "          metrics = evaluate_predictions({k: answers[k]}, {k: all_predictions[k]})\n",
        "          print(json.dumps(metrics))\n",
        "      \"\"\"\n"
      ],
      "metadata": {
        "id": "4Kzd75bkUTJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('em',em)\n",
        "print('tk',tk)\n",
        "print('f1',f1)"
      ],
      "metadata": {
        "id": "eh433xyI61q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "em [0.0, 6.0, 16.666666666666668, 22.666666666666668, 21.333333333333332, 17.333333333333332, 10.0, 4.0, 2.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.3333333333333333, 0.6666666666666666, 1.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]\n",
        "tk [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 40, 50]\n",
        "f1 [0.020833333333333332, 15.39861111111111, 34.326172438672415, 44.17516788766787, 46.310329022829, 45.94180125430125, 44.630497742997704, 40.63119149369145, 37.634488196988165, 34.20407384010324, 29.894118819776697, 26.823178809208216, 24.60567481063679, 22.43074010270301, 19.86675099408612, 16.641160022233066, 15.1014561754907, 13.43372937169635, 13.721398427479853]\n",
        "\n"
      ],
      "metadata": {
        "id": "pWu75QtE-cLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fontsize = 12\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(tk, em, label='EM', marker='o')\n",
        "plt.plot(tk, f1, label='F1', marker='o')\n",
        "\n",
        "plt.xlabel('Generated answer length (# of new tokens)', fontsize=fontsize)\n",
        "plt.ylabel('Score', fontsize=fontsize)\n",
        "\n",
        "plt.xticks([1, 4, 7, 9, 12, 16, 20, 25, 30,40,50], fontsize=fontsize)\n",
        "\n",
        "plt.yticks(fontsize=fontsize)\n",
        "\n",
        "plt.xlim(0)\n",
        "\n",
        "plt.legend(fontsize=fontsize)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IgPaFd2fV881"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U wandb --progress-bar off\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "id": "Sn6yD9ehPM99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off\n",
        "!pip install -q -U bitsandbytes --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git --progress-bar off"
      ],
      "metadata": {
        "id": "N039a-2pPSqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# eval tokenizer does not have eos token and padding\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        "    #truncate=True,\n",
        "    #padding_side=\"left\", # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
        ")\n",
        "#tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "2iH7F4S5QF10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "split = \"validation\"#\"test\"#\n",
        "mrqa_eval = load_dataset(\"enriquesaou/mrqa-squadded-sample\", split=split)"
      ],
      "metadata": {
        "id": "pE4F3Dnu8_xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/mrqa/MRQA-Shared-Task-2019/blob/master/mrqa_official_eval.py\n",
        "\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_predictions(prediction_file):\n",
        "    with open(prediction_file) as f:\n",
        "        predictions = json.load(f)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def read_answers(gold_file):\n",
        "    answers = {}\n",
        "    with gzip.open(gold_file, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = json.loads(line)\n",
        "            if i == 0 and 'header' in example:\n",
        "                continue\n",
        "            for qa in example['qas']:\n",
        "                answers[qa['qid']] = qa['answers']\n",
        "    return answers\n",
        "\n",
        "\n",
        "def evaluate_predictions(answers, predictions, skip_no_answer=False):\n",
        "    f1 = exact_match = total = 0\n",
        "\n",
        "    for qid, ground_truths in answers.items():\n",
        "        if qid not in predictions:\n",
        "            if not skip_no_answer:\n",
        "                message = 'Unanswered question %s will receive score 0.' % qid\n",
        "                print(message)\n",
        "                total += 1\n",
        "            continue\n",
        "        total += 1\n",
        "        prediction = predictions[qid]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "t5EQPyTf9JKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cqa(context, question):\n",
        "    return \"Answer the question extracting from the context below.\\nContext: \" + context + \"\\nQuestion: \" + question + \"\\nAnswer: \""
      ],
      "metadata": {
        "id": "IJLlvqN3Xqn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_phi3(context, question): # not good enough\n",
        "    cq = \"Answer the question extracting from the context below.\\nContext: \" + context + \"\\nQuestion: \" + question\n",
        "    return \"<|user|>\\n\" + cq + \"<|end|>\\n<|assistant|>\""
      ],
      "metadata": {
        "id": "nn8tlNFdeNPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_generate(test_model, prompt, new_tokens=16):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=new_tokens)\n",
        "        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "PI17VKLWXhpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_evaluate = [\"enriquesaou/phi-3-mrqa\"]#[base_model_id]"
      ],
      "metadata": {
        "id": "LbmaJ_F-yY0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "new_tok = 5\n",
        "\n",
        "for model_id in models_to_evaluate:\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16, # to fit into T4\n",
        "    )\n",
        "\n",
        "    # load peft if not base\n",
        "    if model_id != base_model_id:\n",
        "      # note that base_model may be modified in place\n",
        "      model_for_eval = PeftModel.from_pretrained(base_model, model_id)\n",
        "    else: model_for_eval = base_model\n",
        "\n",
        "    model_for_eval.eval()\n",
        "\n",
        "    all_predictions = {}\n",
        "    for example in tqdm(mrqa_eval):\n",
        "      prompt = format_cqa(example['context'], example['question'])\n",
        "      if len(prompt) > 5000: continue # OOM fix\n",
        "      outs = tokenize_and_generate(model_for_eval, prompt, new_tokens=new_tok)\n",
        "      outs = outs.replace(prompt, '')\n",
        "      outs = outs.split('Answer:')[1] if 'Answer:' in outs else outs\n",
        "      all_predictions[example['id']] = outs.strip()\n",
        "\n",
        "\n",
        "    # compute metrics\n",
        "    answers = mrqa_eval.to_dict()\n",
        "    answers = {id: aws['text'] for id, aws in zip(answers['id'], answers['answers'])}\n",
        "    metrics = evaluate_predictions(answers, predictions=all_predictions)\n",
        "\n",
        "    print(model_id, split, json.dumps(metrics))\n",
        "\n",
        "    \"\"\"\n",
        "    for k in answers.keys():\n",
        "        print(all_predictions[k], answers[k])\n",
        "        metrics = evaluate_predictions({k: answers[k]}, {k: all_predictions[k]})\n",
        "        print(json.dumps(metrics))\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "e0gogRim9PZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with phi3 format\n",
        "predis = {}\n",
        "for k in answers.keys():\n",
        "        pred = all_predictions[k].split('<|assistant|>')[1]\n",
        "        print(pred, answers[k])\n",
        "        #metrics = evaluate_predictions({k: answers[k]}, {k: pred})\n",
        "        #print(json.dumps(metrics))\n",
        "        predis[k] = pred\n",
        "\n",
        "metrics = evaluate_predictions(answers, predictions=predis)\n",
        "print(model_id, split, json.dumps(metrics))"
      ],
      "metadata": {
        "id": "Eb3V_SUwlNFF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
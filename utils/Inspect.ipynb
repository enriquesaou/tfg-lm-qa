{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShnyaG5PSeVk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git --progress-bar off\n",
        "!pip install datasets evaluate --progress-bar off\n",
        "!pip install -q -U bitsandbytes --progress-bar off\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git --progress-bar off"
      ],
      "metadata": {
        "id": "FYfs-mPeSl02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://github.com/mrqa/MRQA-Shared-Task-2019/blob/master/mrqa_official_eval.py\n",
        "\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_predictions(prediction_file):\n",
        "    with open(prediction_file) as f:\n",
        "        predictions = json.load(f)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def read_answers(gold_file):\n",
        "    answers = {}\n",
        "    with gzip.open(gold_file, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            example = json.loads(line)\n",
        "            if i == 0 and 'header' in example:\n",
        "                continue\n",
        "            for qa in example['qas']:\n",
        "                answers[qa['qid']] = qa['answers']\n",
        "    return answers\n",
        "\n",
        "\n",
        "def evaluate_predictions(answers, predictions, skip_no_answer=False):\n",
        "    f1 = exact_match = total = 0\n",
        "\n",
        "    for qid, ground_truths in answers.items():\n",
        "        if qid not in predictions:\n",
        "            if not skip_no_answer:\n",
        "                message = 'Unanswered question %s will receive score 0.' % qid\n",
        "                print(message)\n",
        "                total += 1\n",
        "            continue\n",
        "        total += 1\n",
        "        prediction = predictions[qid]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "SY_M3NOZUWQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'subset': \"HotpotQA\",\n",
        "           'id': \"62b2c06aa9b04b1a804341a512564410\",\n",
        "           'question': \"While at the University of Michigan, Louis Smith played with what American jazz trumpeter, bandleader, and composer?\",\n",
        "           'answers': { \"answer_start\": [399], \"text\": [\"Miles Dewey Davis III\"] },\n",
        "           'context': \"[PAR] [TLE] Louis Smith (musician) [SEP] While studying at the University of Michigan, he played with visiting musicians such as Dizzy Gillespie, Miles Davis, Thad Jones and Billy Mitchell, before going on to play with Sonny Stitt, Count Basie and Al McKibbon, Cannonball Adderley, Percy Heath, Philly Joe Jones, Lou Donaldson, Donald Byrd, Kenny Dorham and Zoot Sims. [PAR] [TLE] Miles Davis [SEP] Miles Dewey Davis III (May 26, 1926September 28, 1991) was an American jazz trumpeter, bandleader, and composer. He is among the most influential and acclaimed figures in the history of jazz and 20th century music. Davis adopted a variety of musical directions in his five-decade career which kept him at the forefront of a number of major stylistic developments in jazz.\",\n",
        "           }\n",
        "\n",
        "example_answer = {example['id']: example['answers']['text']}"
      ],
      "metadata": {
        "id": "L9JiRXAIXyN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'subset': \"RelationExtraction\",\n",
        "           'id': \"efc04fd3b51042c7a6e68dd1dd0af8ba\",\n",
        "           'question': \"What year did 52nd government of Turkey start?\",\n",
        "           'answers': { \"answer_start\": [ 42 ], \"text\": [ \"1995\" ] },\n",
        "           'context': \"The 52nd government of Turkey (30 October 1995 -- 6 March 1996) was a caretaker coalition government formed by True Path Party (DYP) and Republican People's Party (CHP).\",\n",
        "           }\n",
        "\n",
        "example_answer = {example['id']: example['answers']['text']}"
      ],
      "metadata": {
        "id": "qdPLekrnYoH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'subset': \"SQuAD\",\n",
        "           'id': \"2654518b7c4d411ba2e45daa9c9e870e\",\n",
        "           'question': \"Who opened the new Parliament building on October 9, 2004?\",\n",
        "           'answers': { \"answer_start\": [743], \"text\": [ \"Queen Elizabeth II\"] },\n",
        "           'context': \"Since September 2004, the official home of the Scottish Parliament has been a new Scottish Parliament Building, in the Holyrood area of Edinburgh. The Scottish Parliament building was designed by Spanish architect Enric Miralles in partnership with local Edinburgh Architecture firm RMJM which was led by Design Principal Tony Kettle. Some of the principal features of the complex include leaf-shaped buildings, a grass-roofed branch merging into adjacent parkland and gabion walls formed from the stones of previous buildings. Throughout the building there are many repeated motifs, such as shapes based on Raeburn's Skating Minister. Crow-stepped gables and the upturned boat skylights of the Garden Lobby, complete the unique architecture. Queen Elizabeth II opened the new building on 9 October 2004.\",\n",
        "           }\n",
        "\n",
        "example_answer = {example['id']: example['answers']['text']}"
      ],
      "metadata": {
        "id": "0FLwPr_HUnbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'subset': \"TextbookQA\",\n",
        "           'id': \"ede8e61b8ea94f4d99a0263b45e2a6b7\",\n",
        "           'question': \"hydrocarbons in gas form are called __________________.\",\n",
        "           'answers': { \"answer_start\": [ 1357 ], \"text\": [ \"natural gas\" ] },\n",
        "           'context': \"Can you name some fossils? How about dinosaur bones or dinosaur footprints? Animal skeletons, teeth, shells, coprolites (otherwise known as feces), or any other remains or traces from a living creature that becomes rock is a fossil. The same processes that formed these fossils also created some of our most important energy resources, fossil fuels. Coal, oil, and natural gas are fossil fuels. Fossil fuels come from living matter starting about 500 million years ago. Millions of years ago, plants used energy from the Sun to form sugars, carbohydrates, and other energy-rich carbon compounds. As plants and animals died, their remains settled on the ground on land and in swamps, lakes, and seas (Figure 1.1). Over time, layer upon layer of these remains accumulated. Eventually, the layers were buried so deeply that they were crushed by an enormous mass of earth. The weight of this earth pressing down on these plant and animal remains created intense heat and pressure. After millions of years of heat and pressure, the material in these layers turned into chemicals called hydrocarbons (Figure 1.2). Hydrocarbons are made of carbon and hydrogen atoms. This molecule with one carbon and four hydrogen atoms is methane. Hydrocarbons can be solid, liquid, or gaseous. The solid form is what we know as coal. The liquid form is petroleum, or crude oil. Natural gas is the gaseous form. The solar energy stored in fossil fuels is a rich source of energy. Although fossil fuels provide very high quality energy, they are non-renewable. Click image to the left or use the URL below. URL:\",\n",
        "           }\n",
        "\n",
        "example_answer = {example['id']: example['answers']['text']}"
      ],
      "metadata": {
        "id": "V_tLUMcUe65B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = {'subset': \"DROP\",\n",
        "           'id': \"21c3bcfcfd0b4f49b0663f8325b852a9\",\n",
        "           'question': \"What happened first, the end of the rebellion of the city of Danzig or Maximilian's II death?\",\n",
        "           'answers': { \"answer_start\": [ 446, 446, 446, 446 ], \"text\": [ \"Maximilian's II death\", \"Maximilian's II death\", \"Maximilian's II death\", \"Maximilian's II death\" ] },\n",
        "           'context': \"The rebellion of the city of Danzig was a revolt from December 1575 to December 1577 of the city against the outcome of the Polish-Lithuanian royal election, 1576. The Polish throne was contested by Stephen Báthory and the Holy Roman Emperor Maximillian II. It began on 12 December 1575 when Emperor Maximillian was chosen as monarch by the Polish Senate, while the majority of the szlachta had voted for Bathory. It ended on 16 December 1577. Maximilian's II death in fall of 1576 weakened Danzig's position and made the conflict less about the recognition of the ruler than about Danzig's privileges. With neither side being able to defeat the other militarily, a compromise was reached, with economic as well as religious privileges of the city being restored and recognized, in return for a large reparation and recognition of Bathory as the king.\",\n",
        "           }\n",
        "\n",
        "example_answer = {example['id']: example['answers']['text']}"
      ],
      "metadata": {
        "id": "QRpnvW2jY-H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phi-3"
      ],
      "metadata": {
        "id": "o-Wn3J1rTJYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model_id = \"enriquesaou/phi-3-mrqa\""
      ],
      "metadata": {
        "id": "sUPdvU0YWOg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16, # to fit into T4\n",
        ")\n",
        "\n",
        "# load peft. note that base_model may be modified in place\n",
        "model = PeftModel.from_pretrained(base_model, model_id)"
      ],
      "metadata": {
        "id": "6lw1FwpYT1hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cqa(context, question):\n",
        "    return \"Answer the question extracting from the context below.\\nContext: \" + context + \"\\nQuestion: \" + question + \"\\nAnswer: \""
      ],
      "metadata": {
        "id": "3s5dZQ9WTqfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_generate(test_model, prompt, new_tokens=16):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=new_tokens)\n",
        "        answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "wYxNM_-HTsRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tok = 5\n",
        "\n",
        "prompt = format_cqa(example['context'], example['question'])\n",
        "outs = tokenize_and_generate(model, prompt, new_tokens=new_tok)\n",
        "outs = outs.replace(prompt, '')\n",
        "outs = outs.split('Answer:')[1] if 'Answer:' in outs else outs\n",
        "outs.strip()\n",
        "\n",
        "# compute metrics\n",
        "metrics = evaluate_predictions(example_answer,\n",
        "                               predictions={example['id']: outs})\n",
        "\n",
        "print(model_id)\n",
        "print(\"=== Original Answer ====\")\n",
        "print(' | '.join(example['answers']['text']))\n",
        "print(\"====== Prediction ======\")\n",
        "print(outs)\n",
        "print(\"========================\")\n",
        "print(json.dumps(metrics))"
      ],
      "metadata": {
        "id": "jkdXTmdOTb8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "723atSidvxu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"google/flan-t5-base\"\n",
        "model_id = \"enriquesaou/flan-t5-base-mrqa\""
      ],
      "metadata": {
        "id": "j9I0SLOHyKEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "tokenizer.add_special_tokens({'sep_token': \"<s>\"})"
      ],
      "metadata": {
        "id": "gECm--DYzJd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "max_length = 512\n",
        "stride = 128\n",
        "\n",
        "def generate_input(_question, _context):\n",
        "    return \" \".join([\"question:\", _question.strip(), tokenizer.sep_token, \"context:\", _context.strip(), tokenizer.sep_token,  \"answer:\"])\n",
        "\n",
        "def preprocess_mrqa_batch(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "        answers = examples[\"answers\"]\n",
        "\n",
        "        inputs = [generate_input(question, context) for question, context in zip(questions, contexts)]\n",
        "        targets = [answer['text'][0] if len(answer) > 0 else \"\" for answer in answers]\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "# validation preprocessing\n",
        "def preprocess_validation(examples):\n",
        "    inputs, targets = preprocess_mrqa_batch(examples)\n",
        "\n",
        "    model_inputs = tokenizer(inputs,\n",
        "                             max_length=max_length,\n",
        "                             stride=stride,\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             return_overflowing_tokens=True,\n",
        "                             return_offsets_mapping=True)\n",
        "    labels = tokenizer(text_target=targets,\n",
        "                       max_length=max_length,\n",
        "                       stride=stride,\n",
        "                       padding=\"max_length\",\n",
        "                       truncation=True)\n",
        "\n",
        "    # Replace tokenizer.pad_token_id in the labels to ignore padding in the loss\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    # examples with long context give us several features -> map feature to example\n",
        "    sample_mapping = model_inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # convert predictions to substrings of the context for evaluation\n",
        "    model_inputs[\"example_id\"] = []\n",
        "    # Augment the overflowing tokens to the labels\n",
        "    labels_out = []\n",
        "    for i in range(len(model_inputs[\"input_ids\"])):\n",
        "        # an example can give many spans -> take index of the example containing the span\n",
        "        sample_index = sample_mapping[i]\n",
        "        model_inputs[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "        labels_out.append(labels[\"input_ids\"][sample_index])\n",
        "\n",
        "    model_inputs[\"labels\"] = labels_out\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, predictions):\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "    # Replace -100s used for padding as we can't decode them\n",
        "    #predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    predictions = [np.where(p != -100, p, tokenizer.pad_token_id) for p in predictions]\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    feature_per_example = {example_id_to_index[feature[\"example_id\"]]: i for i, feature in enumerate(features)}\n",
        "    all_predictions = {}\n",
        "    for example_index, example in enumerate(examples):\n",
        "        # This is the index of the feature associated to the current example.\n",
        "        feature_index = feature_per_example[example_index]\n",
        "        all_predictions[example[\"id\"]] = decoded_preds[feature_index]\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "Y_kEeLfjx6jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "input = preprocess_validation({k: [v] for k,v in example.items()})\n",
        "outs = model.generate(input_ids=torch.tensor(input['input_ids']),\n",
        "                                attention_mask=torch.tensor(input['attention_mask']),\n",
        "                                max_new_tokens=16)\n",
        "\n",
        "\n",
        "# postprocess the prediction\n",
        "prediction = [np.where(p != -100, p, tokenizer.pad_token_id) for p in outs]\n",
        "decoded_pred = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "\n",
        "# compute metrics\n",
        "metrics = evaluate_predictions(example_answer,\n",
        "                               predictions={example['id']: decoded_pred})\n",
        "\n",
        "print(model_id)\n",
        "print(\"=== Original Answer ====\")\n",
        "print(' | '.join(example['answers']['text']))\n",
        "print(\"====== Prediction ======\")\n",
        "print(decoded_pred)\n",
        "print(\"========================\")\n",
        "print(json.dumps(metrics))"
      ],
      "metadata": {
        "id": "-tJ4MIrTvxci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa"
      ],
      "metadata": {
        "id": "QtqfwP2Dvzrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"VMware/roberta-base\"\n",
        "model_id = \"enriquesaou/roberta-vmw-mrqa\""
      ],
      "metadata": {
        "id": "NnaooDZU5DpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_pipeline = pipeline('question-answering', model=model_id, device_map=\"auto\")\n",
        "outs = qa_pipeline(question=example['question'], context=example['context'])\n",
        "\n",
        "# compute metrics\n",
        "metrics = evaluate_predictions(example_answer,\n",
        "                               predictions={example['id']: outs['answer']})\n",
        "\n",
        "print(model_id)\n",
        "print(\"=== Original Answer ====\")\n",
        "print(' | '.join(example['answers']['text']))\n",
        "print(\"====== Prediction ======\")\n",
        "print(outs['answer'])\n",
        "print(\"========================\")\n",
        "print(json.dumps(metrics))"
      ],
      "metadata": {
        "id": "cNUaKtCRv2la"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}